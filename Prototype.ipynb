{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prototype",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/austinm21/Finalproject0.1/blob/master/Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y5OtIrnmhA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_LOW7T-mnjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lbrKcZtyLEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os1pj0hKnF8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXFbA2WdnZE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-34-RyUQoIbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip show tflearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prmUQisuxetB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tflearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JG7pabuzeu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iOpcRua2RQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bztlnLy8sobS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') \n",
        "from nltk.stem.lancaster import LancasterStemmer \n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import numpy\n",
        "#import tflearn\n",
        "import random \n",
        "import json \n",
        "import tensorflow\n",
        "import pickle\n",
        "\n",
        "\n",
        "with open(\"intents.json\") as file:\n",
        "  data = json.load(file) \n",
        "#try:\n",
        "  with open(\"data.pickle\", \"rb\") as f: \n",
        "    words, labels, training, output = pickle.load(f)\n",
        "\n",
        "#except: \n",
        "  words = []\n",
        "  labels = []\n",
        "  docs_x = [] \n",
        "  docs_y = []\n",
        "\n",
        "  for intent in data[\"intents\"]: \n",
        "    for pattern in intent[\"patterns\"]:\n",
        "      wrds = nltk.word_tokenize(pattern)\n",
        "      words.extend(wrds)\n",
        "      docs_x.append(wrds)\n",
        "      docs_y.append(intent[\"tag\"])\n",
        "      \n",
        "      if intent[\"tag\"] not in labels: \n",
        "        labels.append(intent[\"tag\"])\n",
        "\n",
        "  words = [stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
        "  words = sorted(list(set(words))) \n",
        "\n",
        "  labels = sorted (labels)\n",
        "\n",
        "  training = []\n",
        "  output = []\n",
        "\n",
        "  out_empty = [0 for _ in range(len(labels))] \n",
        "\n",
        "  for x, doc in enumerate(docs_x):\n",
        "    bag = [] \n",
        "\n",
        "    wrds = [stemmer.stem(w) for w in doc] \n",
        "\n",
        "    for w in words: \n",
        "      if w in wrds:\n",
        "        bag.append(1)\n",
        "      else: \n",
        "        bag.append(0) \n",
        "    output_row = out_empty[:] \n",
        "    output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "  training = numpy.array(training) \n",
        "  output = numpy.array(output) \n",
        "\n",
        "  with open(\"data.pickle\", \"wb\") as f: \n",
        "    pickle.dump((words, labels, training, output), f) \n",
        "\n",
        "#tensorflow.reset_default_graph() \n",
        "\n",
        "#lit = tflearn.input_data(shape=[None, len(training[0])]) \n",
        "#lit = tflearn.fully_connected(lit, 8)\n",
        "#lit = tflearn.fully_connected(lit, 8) \n",
        "#lit = tflearn.fully_connected(lit, len(output[0]), activation =\"softmax\")  \n",
        "#lit = tflearn.regression(lit)\n",
        "\n",
        "#model = tflearn.DNN(lit) \n",
        "\n",
        "#try: \n",
        "  #model.load(\"model.tflearn\")\n",
        "#except:\n",
        "  #model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "  #model.save(\"model.tflearn\")  \n",
        "\n",
        "def bag_of_words(s, words): \n",
        "  bag = [0 for _ in range(len(words))]\n",
        "\n",
        "  s_words = nltk.word_tokenize(s)\n",
        "  s_words = [stemmer.stem(word.lower()) for word in s_words] \n",
        "\n",
        "  for se in s_words:\n",
        "    for i, w in enumerate(words): \n",
        "      if w == se: \n",
        "        bag[i].append(1) \n",
        "  \n",
        "  return numpy.array(bag) \n",
        "\n",
        "def chat():\n",
        "  print(\"Start talking with the homie! (type quit to stop) \")\n",
        "  while True:\n",
        "    inp = input(\"You: \")\n",
        "    if inp.lower() == \"quit\": \n",
        "      break \n",
        "    \n",
        "    #results = model.predicit([bag_of_words(inp, words)]) \n",
        "    #results_index = numpy.argmax(results)\n",
        "    #tag = labels[results_index]\n",
        "    #print(tag) \n",
        "\n",
        "chat() \n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}